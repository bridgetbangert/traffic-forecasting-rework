{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f71f99d-33b8-4cd6-b2b4-7610d44f269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "761ad3f5-ae73-4f0d-82c4-2b6fc518e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./AzureKeyVault.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9292b6b8-0574-48da-94ab-c20f0bb9f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c4f97b9-7ead-4d72-8282-a12321f46b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/05 11:22:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TrafficForecasting\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.default.parallelism\", \"200\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1cde2b9d-c392-434b-86d7-04583360a137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class USCensusAPI:\n",
    "    def __init__(self, dataset, cols, location):\n",
    "        self.host = 'https://api.census.gov/data'\n",
    "        self.dataset = dataset\n",
    "        self.cols = variables\n",
    "        self.location = location\n",
    "        self.user_key = KeyVault().get_secret(\"CensusAPIKey\")\n",
    "\n",
    "    def get_query_url(self):\n",
    "        return f\"{self.host}/{self.dataset}?get={self.cols}&for={self.location}&key={self.user_key}\"\n",
    "\n",
    "    def query_results(self):\n",
    "        return requests.get(self.get_query_url()).text\n",
    "\n",
    "    def query_output(self):\n",
    "        print(self.query_results())\n",
    "\n",
    "    def query_df(self):\n",
    "        data = json.loads(self.query_results())\n",
    "        return pd.DataFrame(\n",
    "            data[1:],\n",
    "            columns=data[0]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c0e0cd93-431d-49ec-9bee-19eb423d29a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AustinDataAPI:\n",
    "    def __init__(self, filter=None, limit=None, page=None):\n",
    "        self.url = 'https://data.austintexas.gov/resource/dx9v-zd7x.json'\n",
    "        self.app_token = KeyVault().get_secret(\"AustinDataToken\")\n",
    "        self.filter = filter\n",
    "        self.limit = limit\n",
    "        self.page = page\n",
    "\n",
    "    def query_results(self, offset=None):\n",
    "        params = {\"$$app_token\": self.app_token}\n",
    "        \n",
    "        if self.filter:\n",
    "            params.update({\"$where\": self.filter})\n",
    "        if self.limit:\n",
    "            params.update({\"limit\": self.limit})\n",
    "        if self.page:\n",
    "            params.update({\"page\": self.page})\n",
    "        if offset:\n",
    "            params.update({\"$offset\": offset})\n",
    "            \n",
    "        response = requests.get(self.url, params=params)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "    def query_df(self):\n",
    "        return pd.DataFrame.from_records(self.query_results().json())\n",
    "\n",
    "    def get_total_records(self):\n",
    "        params = { \"$select\": \"count(*)\" }        \n",
    "        response = requests.get(self.url, params=params)\n",
    "        response.raise_for_status()\n",
    "        return int(response.json()[0]['count'])\n",
    "\n",
    "    def get_flattened_data(self, batch_size=1000):\n",
    "        total_records = self.get_total_records()\n",
    "        logger.info(f\"Total records to process: {total_records}.\")\n",
    "        offsets = range(0, self.get_total_records(), batch_size)\n",
    "        \n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = []\n",
    "            for offset, result in zip(offsets, executor.map(self.query_results, offsets)):\n",
    "                logger.info(f\"Processed batch starting at offset {offset}.\")\n",
    "                results.extend(result)\n",
    "\n",
    "        logger.info(\"Processing has been complete.\")\n",
    "        return results\n",
    "\n",
    "    def get_spark_df(self, batch_size=1000):\n",
    "        logger.info(\"Creating spark dataframe for Austin API data.\")\n",
    "        return spark.createDataFrame(self.get_flattened_data())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
